# 神经网络与深度学习2
## 为什么要“深度学习”
全连接网络：链接权过多，算的慢，难收敛，同时可能进入局部极小值，也容易产生过拟合问题。例如：输入为1000×1000图像，隐含层有1M个节点，则输入->隐含层间有1×1012数量级参数。

为了解决算的慢问题：减少权值连接，每一个节点只连到上一层的少数神经元，即局部连接网络。

深度学习平台简介
|     库名      |   发布者    |              支持语言              |             支持系统              |
| :-----------: | :---------: | :--------------------------------: | :-------------------------------: |
|  TensorFlow   |   Google    |         Python/C++/Java/Go         |     Linux/Mac OS/Android/iOS      |
|     Caffe     | UC Berkeley |         Python/C++/MATLAB          |       Linux/Mac OS/Windows        |
|      JAX      |   Google    |               Python               |           Linux/Windows           |
|     MXNet     | Amazon/DMLC | Python/C++/MATLAB/Julia/Go/R/Scala | Linux/Mac OS/ Windows/Android/iOS |
| Torch/PyTorch |  Facebook   |            C/Python/...            | Linux/Mac OS/ Windows/Android/iOS |
| PaddlePaddle  |    百度     |               Python               |           Linux/Windows           |
|  MMdetection  | 商汤/港中文 |               Python               |           Linux/Windows           |

## 卷积神经网络基础

卷积神经网络（Convolutional Neural Network, CNN）是深度学习中专门处理网格状数据（如图像、音频、视频）的神经网络架构。其核心思想是通过局部感知和权值共享高效提取空间特征。

 ### 核心组件
1.卷积层（Convolution Layer）
作用：提取局部特征（如边缘、纹理）。

关键概念：

滤波器（Filter/Kernel）：小的权重矩阵（如3×3、5×5），在输入上滑动进行点积运算。

特征图（Feature Map）：滤波器与输入局部区域计算后的输出结果。

步长（Stride）：滤波器每次滑动的像素数（步长越大，输出尺寸越小）。

填充（Padding）：在输入边缘补零，控制输出尺寸（如same padding保持尺寸不变）。

特点：局部连接（减少参数）、权值共享（同一滤波器扫描全图）。

2.激活函数（Activation Function）
作用：引入非线性，增强模型表达能力。

常用函数：

ReLU：

$$f(x)=\max(0,x)（缓解梯度消失，计算高效）$$

Leaky ReLU/Swish：改进ReLU的负区间处理。

3.池化层（Pooling Layer）
作用：降维、减少参数、增强平移不变性。

类型：

最大池化（Max Pooling）：取局部区域最大值（保留显著特征）。

平均池化（Average Pooling）：取局部区域平均值（平滑特征）。

4.全连接层（Fully Connected Layer）
作用：将提取的特征映射到最终输出（如分类概率）。

位置：通常位于网络末端，连接所有神经元。

### CNN的优势
参数效率：通过局部连接和权值共享，参数数量远少于全连接网络。

平移不变性：滤波器扫描全图，可识别不同位置的特征。

层次化特征提取：

浅层：边缘、颜色等低级特征。

深层：纹理、物体部件等高级语义特征。

### 经典网络结构
LeNet-5（1998）：首个成功应用于手写数字识别的CNN，包含卷积、池化、全连接层。

AlexNet（2012）：引入ReLU、Dropout，赢得ImageNet竞赛。

VGGNet（2014）：通过堆叠3×3小卷积核构建深层网络。

ResNet（2015）：残差连接（Residual Block）解决梯度消失，支持超深层网络（如ResNet-152）。

### 关键超参数
滤波器数量：决定输出特征图的通道数。

滤波器尺寸：常见3×3、5×5。

步长（Stride）：影响输出尺寸和计算量。

填充（Padding）：控制边缘信息保留。

### 应用场景
图像分类（如ResNet、EfficientNet）。

目标检测（如Faster R-CNN、YOLO）。

语义分割（如U-Net）。

其他领域：视频分析、医学图像处理、自然语言处理（文本分类）。

### 代码示例（PyTorch）

```
import torch.nn as nn

class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)  # 输入3通道，输出16通道
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool2d(2, 2)  # 输出尺寸减半
        self.fc = nn.Linear(16*16*16, 10)  # 假设输入图像为32x32

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))  # 输出尺寸：16x16x16
        x = x.view(x.size(0), -1)  # 展平
        x = self.fc(x)
        return x
```

## 经典卷积神经网络结构

以下是几种经典卷积神经网络（CNN）结构的详细介绍，包括它们的核心设计、创新点及对深度学习发展的影响：

1.LeNet-5（1998）
提出者：Yann LeCun等，用于手写数字识别（MNIST数据集）。

结构：

输入：32×32灰度图像。

2个卷积层（5×5滤波器）+ 2个池化层（平均池化）。

3个全连接层（输出10类概率）。

创新点：

首次成功将卷积、池化、全连接结合，奠定CNN基础。

证明局部感知和权值共享的有效性。

意义：标志着CNN的诞生，但因算力限制未广泛应用。

2.AlexNet（2012）
提出者：Alex Krizhevsky等，赢得ImageNet竞赛（Top-5错误率15.3%）。

结构：

输入：227×227 RGB图像。

5个卷积层（大尺寸滤波器如11×11）+ 3个池化层（最大池化）。

3个全连接层（输出1000类概率）。

创新点：

ReLU激活函数：缓解梯度消失，加速训练。

Dropout：在全连接层随机失活神经元，防止过拟合。

GPU并行训练：首次利用双GPU加速模型训练。

意义：引发深度学习热潮，推动CNN在视觉任务中的广泛应用。

3.VGGNet（2014）
提出者：牛津大学Visual Geometry Group（VGG）。

结构：

输入：224×224 RGB图像。

核心设计：重复堆叠3×3卷积层（代替大尺寸滤波器）。

典型配置：VGG-16（13卷积层 + 3全连接层）。

创新点：

小卷积核策略：多个3×3卷积堆叠等效于更大感受野（如3层3×3 ≈ 1层7×7），但参数更少、非线性更强。

统一模块化设计：每级网络通过池化层压缩特征图尺寸。

意义：验证了深度对模型性能的重要性，结构简洁易扩展，成为后续模型的参考基准。

4.ResNet（2015）
提出者：何恺明等（Microsoft Research），ImageNet竞赛冠军（Top-5错误率3.57%）。

结构：

输入：224×224 RGB图像。

核心设计：残差块（Residual Block），包含跳跃连接（Shortcut Connection）。

典型配置：ResNet-50、ResNet-152。

创新点：

残差学习：通过跳跃连接传递原始输入，解决深层网络梯度消失/爆炸问题。

公式：输出 = F(x) + x，网络只需学习残差F(x) = 输出 - x。

Bottleneck结构：用1×1卷积降维后再升维（减少计算量）。

意义：首次成功训练超深层网络（100+层），推动模型深度飞跃，成为现代CNN的标配组件。
