# 神经网络与深度学习2
## 为什么要“深度学习”
全连接网络：链接权过多，算的慢，难收敛，同时可能进入局部极小值，也容易产生过拟合问题。例如：输入为1000×1000图像，隐含层有1M个节点，则输入->隐含层间有1×1012数量级参数。

为了解决算的慢问题：减少权值连接，每一个节点只连到上一层的少数神经元，即局部连接网络。

深度学习平台简介
|     库名      |   发布者    |              支持语言              |             支持系统              |
| :-----------: | :---------: | :--------------------------------: | :-------------------------------: |
|  TensorFlow   |   Google    |         Python/C++/Java/Go         |     Linux/Mac OS/Android/iOS      |
|     Caffe     | UC Berkeley |         Python/C++/MATLAB          |       Linux/Mac OS/Windows        |
|      JAX      |   Google    |               Python               |           Linux/Windows           |
|     MXNet     | Amazon/DMLC | Python/C++/MATLAB/Julia/Go/R/Scala | Linux/Mac OS/ Windows/Android/iOS |
| Torch/PyTorch |  Facebook   |            C/Python/...            | Linux/Mac OS/ Windows/Android/iOS |
| PaddlePaddle  |    百度     |               Python               |           Linux/Windows           |
|  MMdetection  | 商汤/港中文 |               Python               |           Linux/Windows           |

## 卷积神经网络基础

卷积神经网络（Convolutional Neural Network, CNN）是深度学习中专门处理网格状数据（如图像、音频、视频）的神经网络架构。其核心思想是通过局部感知和权值共享高效提取空间特征。

 ### 核心组件
1.卷积层（Convolution Layer）
作用：提取局部特征（如边缘、纹理）。

关键概念：

滤波器（Filter/Kernel）：小的权重矩阵（如3×3、5×5），在输入上滑动进行点积运算。

特征图（Feature Map）：滤波器与输入局部区域计算后的输出结果。

步长（Stride）：滤波器每次滑动的像素数（步长越大，输出尺寸越小）。

填充（Padding）：在输入边缘补零，控制输出尺寸（如same padding保持尺寸不变）。

特点：局部连接（减少参数）、权值共享（同一滤波器扫描全图）。

2.激活函数（Activation Function）
作用：引入非线性，增强模型表达能力。

常用函数：

ReLU：
f
(
x
)
=
max
⁡
(
0
,
x
)
f(x)=max(0,x)（缓解梯度消失，计算高效）。

Leaky ReLU/Swish：改进ReLU的负区间处理。

3.池化层（Pooling Layer）
作用：降维、减少参数、增强平移不变性。

类型：

最大池化（Max Pooling）：取局部区域最大值（保留显著特征）。

平均池化（Average Pooling）：取局部区域平均值（平滑特征）。

4.全连接层（Fully Connected Layer）
作用：将提取的特征映射到最终输出（如分类概率）。

位置：通常位于网络末端，连接所有神经元。

### CNN的优势
参数效率：通过局部连接和权值共享，参数数量远少于全连接网络。

平移不变性：滤波器扫描全图，可识别不同位置的特征。

层次化特征提取：

浅层：边缘、颜色等低级特征。

深层：纹理、物体部件等高级语义特征。

### 经典网络结构
LeNet-5（1998）：首个成功应用于手写数字识别的CNN，包含卷积、池化、全连接层。

AlexNet（2012）：引入ReLU、Dropout，赢得ImageNet竞赛。

VGGNet（2014）：通过堆叠3×3小卷积核构建深层网络。

ResNet（2015）：残差连接（Residual Block）解决梯度消失，支持超深层网络（如ResNet-152）。

### 关键超参数
滤波器数量：决定输出特征图的通道数。

滤波器尺寸：常见3×3、5×5。

步长（Stride）：影响输出尺寸和计算量。

填充（Padding）：控制边缘信息保留。

### 应用场景
图像分类（如ResNet、EfficientNet）。

目标检测（如Faster R-CNN、YOLO）。

语义分割（如U-Net）。

其他领域：视频分析、医学图像处理、自然语言处理（文本分类）。

### 代码示例

