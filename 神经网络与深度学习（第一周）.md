# 神经网络与深度学习1

## 多层感知机

多层感知机（Multural-Layer Perceptron, MLP）是一种前馈人工神经网络，由多个神经元层组成。它是单层感知机的扩展，通过引入隐藏层和非线性激活函数，能够解决线性不可分问题（如异或问题），并具有更强的特征表达能力。

### 核心结构：

输入层（Input Layer）：接收原始数据（如图像像素、文本向量等）。每个节点对应一个输入特征。

隐藏层（Hidden Layers）：介于输入层和输出层之间的非线性变换层。可包含多个隐藏层（即“深度”网络）。每个节点通过权重和偏置与前一层的所有节点连接。

输出层（Output Layer）：生成最终预测结果（如分类概率、回归值等）。节点数由任务决定（如二分类用1个节点，多分类用Softmax层）。

### 关键组件：

激活函数（Activation Function）：引入非线性，使网络能拟合复杂函数。

常见激活函数：

ReLU（Rectified Linear Unit）：

$$
f(x) = max(0, x)(缓解梯度消失)
$$

Sigmoid：

$$
f(x) = 1/(1+e^{-x})(输出在(0,1)，适合二分类)
$$

Tanh：

$$
f(x) = (e^x - e^{-x})/(e^x + e^{-x})(输出在(-1,1))
$$

Softmax：多分类输出层，将输出转化为概率分布。

损失函数（Loss Function）：衡量预测值与真实值的差距。

常见选择：均方误差（回归）、交叉熵损失（分类）。

优化算法（Optimizer）：通过反向传播（Backpropagation）更新权重。

常用优化器：随机梯度下降（SGD）、Adam、RMSprop。

### 数学表示
前向传播：对于第
$$\mathrm{l}$$
层，输出计算为：

$$\mathbf{z}^{(l)}=\mathbf{W}^{(l)}\mathbf{a}^{(l-1)}+\mathbf{b}^{(l)}$$ 
$$\mathbf{a}^{(l)}=f(\mathbf{z}^{(l)})$$

其中：
$$\mathbf{w}(l)$$
为权重矩阵，
$$\mathbf{b}^{(l)}$$
为偏置向量，
$$\text{f}$$
为激活函数。

### 优势与局限
优势：
能逼近任意复杂度的连续函数（万能近似定理）。
适用于多种任务（分类、回归等）。

局限：
全连接结构参数量大，易过拟合。
对图像、序列等结构化数据不如CNN、RNN高效。

### 代码示例（PyTorch）
```
import torch.nn as nn

class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(MLP, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
    
    def forward(self, x):
        return self.layers(x)

# 示例：输入维度784（如28x28图像），隐藏层256，输出10类
model = MLP(784, 256, 10)
```

### 应用场景

表格数据分类/回归

简单图像识别（如MNIST）

特征嵌入（Embedding）后的下游任务

## BP算法

BP算法是训练神经网络的核心方法，通过计算损失函数对参数的梯度并更新权重，以最小化预测误差。其过程分为以下步骤：

1.前向传播：
输入数据逐层传递，每层进行线性变换（权重矩阵相乘加偏置）并通过激活函数（如Sigmoid、ReLU）生成输出，直至输出层得到预测值。

2.计算损失：
使用损失函数（如均方误差、交叉熵）衡量预测值与真实值的差距，得到标量损失值。

3.反向传播梯度：

输出层梯度：计算损失对输出层输入的梯度（损失函数导数）。

逐层反向传递：从输出层开始，利用链式法则计算各层参数的梯度。

梯度 = 上一层梯度 × 激活函数导数 × 前一层输出（对权重）或直接传递（对偏置）。

链式法则：将梯度从后向前传递，依次计算各层权重和偏置的梯度。

4.参数更新
使用优化算法（如梯度下降）按学习率调整参数：

$$W=W-\eta\cdot\frac{\partial\mathrm{Loss}}{\partial W},\quad b=b-\eta\cdot\frac{\partial\mathrm{Loss}}{\partial b}$$

其中，
$$\eta$$
为学习率，控制更新步长。

### 关键点

激活函数需可导（如ReLU在非零点可导），否则无法计算梯度。

链式法则是核心，确保高效计算高维参数梯度。

挑战：梯度消失/爆炸（深层网络中梯度可能过小或过大）。

BP算法通过迭代“前向预测→反向调参”优化网络，广泛应用于多层感知机、CNN、RNN等模型训练。



